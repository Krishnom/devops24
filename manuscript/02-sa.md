# Enabling Process Communication With Kube API Through Service Accounts {#sa}

T> Humans are not the only ones who rely on a cluster. Processes in containers often need to invoke Kube API as well. When using RBAC for authentication, we need to decide which users will have permissions to perform specific actions. The same holds true for the processes running inside containers.

When we (humans) try to access a Kubernetes cluster with RBAC enabled, we are authenticated as users. Our username provides an identity that API server uses to decide whether we are allowed to perform intended actions. Similarly processes running inside containers might also need to access the API. In such cases, they are authenticated as a specific ServiceAccount.

ServiceAccounts provide a mechanism to grant permissions to processes running inside containers. In many ways, ServiceAccounts are very similar to RBAC users or groups. With humans, we use RoleBindings and ClusterRoleBindings to relate users and groups to Roles and ClusterRoles. When working with processes, the main difference is in the name and the scope. Instead of users or groups, we create ServiceAccounts which we bind to roles.However unlike users that can be global, ServiceAccounts are tied to specific Namespaces.

I> This chapter assumes that you are already familiar with Namespaces, Ingress, Deployments, Roles, ClusterRoles, RoleBindings, ClusterRoleBindings, Users, and Groups. If you're not, please refer to [The DevOps 2.3 Toolkit: Kubernetes](https://amzn.to/2GvzDjy) for more info.

We won't go into any more theory. Instead, we'll try to learn different aspects of ServiceAccounts through hands-on examples.

## Creating A Cluster

We'll start the hands-on walk-through by entering the directory where we cloned the `vfarcic/k8s-specs` repository.

I> All the commands from this chapter are available in the [02-sa.sh](https://gist.github.com/5fdca8e7f7bb426003abf4ad55745807) Gist.

```bash
cd k8s-specs

git pull
```

Next, we'll need a cluster which we can use to experiment with ServiceAccounts. The requirements are the same as those we used in the previous chapter. We'll need **Kubernetes version 1.9** or higher as well as **nginx Ingress Controller**, **RBAC**, and a **default StorageClass**. If you didn't destroy it, please continue using the cluster you created in the previous chapter. Otherwise, it should be reasonably fast to create a new one. For your convenience, the Gists and the specs we used before are available here as well.

* [docker4mac.sh](https://gist.github.com/06e313db2957e92b1df09fe39c249a14): **Docker for Mac** with 2 CPUs, 2GB RAM, and with nginx Ingress.
* [minikube.sh](https://gist.github.com/536e6329e750b795a882900c09feb13b): **minikube** with 2 CPUs, 2GB RAM, and with `ingress`, `storage-provisioner`, and `default-storageclass` addons enabled.
* [kops.sh](https://gist.github.com/2a3e4ee9cb86d4a5a65cd3e4397f48fd): **kops in AWS** with 3 t2.small masters and 2 t2.medium nodes spread in three availability zones, and with nginx Ingress (assumes that the prerequisites are set through [Appendix B](#appendix-b)).
* [minishift.sh](https://gist.github.com/c9968f23ecb1f7b2ec40c6bcc0e03e4f): **minishift** with 2 CPUs, 2GB RAM, and version 1.16+.
* [gke.sh](https://gist.github.com/5c52c165bf9c5002fedb61f8a5d6a6d1): **Google Kubernetes Engine (GKE)** with 3 n1-standard-1 (1 CPU, 3.75GB RAM) nodes (one in each zone), and with nginx Ingress controller running on top of the "standard" one that comes with GKE. We'll use nginx Ingress for compatibility with other platforms. Feel free to modify the YAML files if you prefer NOT to install nginx Ingress.

Now that we have a cluster, we can proceed with a few examples.

## Configuring Jenkins Kubernetes Plugin

We'll start by creating the same Jenkins StatefulSet we used in the previous chapter. Once it's up-and-running, we'll try to use [Jenkins Kubernetes plugin](https://github.com/jenkinsci/kubernetes-plugin). If we're successful, we'll have a tool which could be used to execute continuous delivery or deployment tasks inside a Kubernetes cluster.

```bash
cat sa/jenkins-no-sa.yml
```

We won't go through the definition since it is the same as the one we used in the previous chapter. There's no mystery that has to be revealed, so we'll move on and create the resources defined in that YAML.

W> ## A note to minishift users
W>
W> OpenShift does not allow setting `fsGroup` in the security context, it uses Routes instead of Ingress, and Services accessible through Routes need to be the `LoadBalancer` type. Due to those changes, I had to prepare a different YAML specification for minishift. Please execute `oc apply -f sa/jenkins-no-sa-oc.yml --record` instead of the command that follows.

```bash
kubectl apply \
    -f sa/jenkins-no-sa.yml \
    --record
```

Next, we'll wait until `jenkins` StatefulSet is rolled out.

```bash
kubectl -n jenkins \
    rollout status sts jenkins
```

W> ## A note to GKE users
W>
W> GKE uses external load balancer as Ingress. To work properly, the `type` of the service related to Ingress needs to be `NodePort`. We'll have to patch the service to change its type. Please execute the command that follows.
W>
W> `kubectl -n jenkins patch svc jenkins -p '{"spec":{"type": "NodePort"}}'`

Next, we'll discover the DNS (or IP) of the load balancer.

W> ## A note to GKE users
W>
W> Please change `hostname` to `ip` in the following command. The `jsonpath` should be `{.status.loadBalancer.ingress[0].ip}`.
W> Please note that GKE Ingress spins up an external load balancer and it might take a while until the IP is generated. Therefore, you might need to repeat the command that follows until you get the IP.

W> ## A note to minikube users
W>
W> Please change the following command to `CLUSTER_DNS=$(minikube ip)`.

W> ## A note to minishift users
W>
W> Please change the following command to `CLUSTER_DNS=jenkins-jenkins.$(minishift ip).nip.io`.

```bash
CLUSTER_DNS=$(kubectl -n jenkins \
    get ing jenkins \
    -o jsonpath="{.status.loadBalancer.ingress[0].hostname}")

echo $CLUSTER_DNS
```

Now that we know the address of the cluster, we can proceed and open Jenkins UI in a browser.

```bash
open "http://$CLUSTER_DNS/jenkins"
```

W> In some cases (e.g., GKE), it might take a few minutes until the external load balancer is created. If you see 40x or 50x error message, please wait for a while and try to open Jenkins in the browser again.

Now we need to go through the setup wizard. It's a dull process, and I'm sure you're not thrilled with the prospect of going through it. However, we're still missing knowledge and tools that will allow us to automate the process. For now, we'll have to do the boring part manually.

The first step is to get the initial admin password.

W> ## A note to Windows users
W>
W> The command that follows might result in `no such file or directory` error. In that case, please replace `/` with `//` in the path. Continue applying the same fix throughout the book if you continue experiencing the same problem.

```bash
kubectl -n jenkins \
    exec jenkins-0 -it -- \
    cat /var/jenkins_home/secrets/initialAdminPassword
```

Please copy the output and paste it into the *Administrator password* field. Click the *Continue* button, followed with a click to *Install suggested plugins* button. Fill in the *Create First Admin User* fields and press the *Save and Finish* button.

Jenkins is ready, and only a click away. Please press the *Start using Jenkins* button.

If we are to use the [Kubernetes plugin](https://github.com/jenkinsci/kubernetes-plugin), we need to install it first. We'll do that through the *available plugins section* of the *plugin manager screen*.

```bash
open "http://$CLUSTER_DNS/jenkins/pluginManager/available"
```

Type *Kubernetes* in the *Filter* field and select the checkbox next to it.

Since we are already in the plugin manager screen, we might just as well install BlueOcean as well. It'll make Jenkins prettier.

Type *BlueOcean* in the *Filter* field and select the checkbox next to it.

Now that we selected the plugins we want, the next step is to install them. Please click the *Install without restart* button and wait until all the plugins (and their dependencies) are installed.

We are not yet finished. We still need to configure the newly installed Kubernetes plugin.

```bash
open "http://$CLUSTER_DNS/jenkins/configure"
```

The plugin adds Kubernetes as yet another *Cloud* provider. Please expand the *Add a new cloud* drop-down list inside the *Cloud* section, and select *Kubernetes*. The section you're looking for should be somewhere close to the bottom of the screen.

Now that we added Kubernetes as a Cloud provider, we should confirm that it works. Please click the *Test Connection* button.

Unless you forgot to unable RBAC in your cluster, the output should be similar to the one that follows.

```
Error testing connection : Failure executing: GET at: https://kubernetes.default.svc/api/v1/namespaces/jenkins/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods is forbidden: User "system:serviceaccount:jenkins:default" cannot list pods in the namespace "jenkins".
```

W> ## A note to Docker For Mac/Windows users
W>
W> Even though Docker for Mac/Windows supports RBAC, it allows any internal process inside containers to communicate with Kube API. That is quite the opposite from what you'll experience in other Kubernetes platforms. As a result, you'll see a few discrepancies between the outputs in this chapter and those you'll see in your cluster. You'll notice that you'll be able to perform actions that are forbidden in other Kubernetes platforms.

API server rejected our request to list the Pods in the `jenkins` Namespace. Such a reaction makes perfect sense. If any process in any container could request anything from the API, our security efforts would be useless. What would be the point of trying to restrict users (humans), if all it takes is to create a single Pod that sends a request to the API server? Kube API rightfully denied our request to list the Pods.

Just as a username provides a sort of identity to humans, a ServiceAccount is an identity for all the processes that run in containers. Since we did not specify any ServiceAccount for the Pod where Jenkins is running, Kubernetes assigned one for us. The Pod is authenticated as the `default` account which happens to be bound to roles that give almost no permissions. In other words, if we do not define a ServiceAccount for a Pod, it'll be associated with the ServiceAccount `default`, and the processes inside that Pod will not be able to requests almost anything from the API server.

The `default` ServiceAccount is created for every Namespace in the cluster. It is, in a way, similar to the default StorageClass. If a Pod does not have a ServiceAccount, it'll get the `default`. That ServiceAccount has insufficient privileges, and it cannot do almost anything. We can bind more permissive role to the `default` ServiceAccount, but that would be a very uncommon solution to the problem.

We need to associate Jenkins process with an entity that has more permissions. As a minimum, we should be able to list the Pods in the `jenkins` Namespace. To do that, we have to learn about ServiceAccounts first.

We'll delete `jenkins` Namespace and search for a simple way explore ServiceAccounts.

```bash
kubectl delete ns jenkins
```

## Exploring the `default` ServiceAccount

Jenkins might not be the best starting point in our exploration of ServiceAccounts. Too many things are happening that are out of our control. There's too much "magic" hidden behind Jenkins code. Instead, we'll start with something simpler. We'll run `kubectl` as a Pod. If we manage to make that work, we should have no problem applying the newly acquired knowledge to Jenkins and other similar use-cases we might have.

Unfortunately, there is no `kubectl` official image (at least not in Docker Hub), so I built one. The definition is in the [vfarcic/kubectl](https://github.com/vfarcic/kubectl) GitHub repository. Let's take a quick look.

```bash
curl https://raw.githubusercontent.com/vfarcic/kubectl/master/Dockerfile
```

The Dockerfile is so uneventful and straightforward that there's probably no need going through it. It's a `kubectl` binary in an `alpine` based image. Not more, not less.

Let's run it.

```bash
kubectl run kubectl \
    --image=vfarcic/kubectl \
    --restart=Never \
    sleep 10000
```

We should wait for a few moments until the image is pulled and the container that forms the Pod is running.

Let's start by checking out the `serviceAccount` entry in the Pod specification.

```bash
kubectl get pod kubectl \
    -o jsonpath="{.spec.serviceAccount}"
```

The output is as follows.

```
default
```

Since we did not specify any ServiceAccount, Kubernetes automatically assigned the `default`. That account was created with the Namespace.

We might be able to dig a bit more information in the `kubectl` container.

```bash
kubectl exec -it kubectl -- sh
```

No matter which ServiceAccount is used, Kubernetes always mounts a secret with the information about that account. The secret is mounted inside the `/var/run/secrets/kubernetes.io/serviceaccount` directory. 

```bash
cd /var/run/secrets/kubernetes.io/serviceaccount

ls -la
```

We entered into the secret's directory and listed all the files. The output is as follows.

```
total 4
... .
... ..
... ..2018_05_07_00_35_25.899750157
... ..data -> ..2018_05_07_00_35_25.899750157
... ca.crt -> ..data/ca.crt
... namespace -> ..data/namespace
... token -> ..data/token
```

We can see that it created three soft-links (`ca.crt`, `namespace`, and `token`) which are pointing to a directory that contains the actual files. You should be able to guess what those files contain. The token and the certificate are used for authentication when communicating with the API server. The third file contains only the Namespace in which the Pod is running.

Let's try a very simple operation.

```bash
kubectl get pods
```

The output is as follows.

```
Error from server (Forbidden): pods is forbidden: User "system:serviceaccount:default:default" cannot list pods in the namespace "default"
```

We got a similar error message as when we tried to use Jenkins' Kubernetes plugin without credentials. Once again we're faced with limited permissions bound to the `default` account. We'll change that soon. For now, we'll exit the container and remove the `kubectl` Pod.

```bash
exit

kubectl delete pod kubectl
```

We saw the limitations of the `default` account. We'll try to overcome them by creating our own ServiceAccounts.

## Creating ServiceAccounts

Let's take a look at service accounts currently available in the `default` Namespace.

```bash
kubectl get sa
```

The output is as follows.

```
NAME    SECRETS AGE
default 1       24m
```

At the moment, there is only one ServiceAccount called `default`. We already saw the limitations of that account. It is stripped from (almost) all the privileges. If we check the other Namespaces, we'll notice that all of them have only the `default` ServiceAccount. Whenever we create a new Namespace, Kubernetes creates that account for us.

W> ## A note to minishift users
W>
W> OpenShift is an exception. Unlike most other Kubernetes flavors, it created a few ServiceAccounts in the `default` Namespace. Feel free to explore them later when you learn more about ServiceAccounts and their relations to Roles.

We already established that we'll need to create new ServiceAccounts if we are ever to allow processes in containers to communicate with Kube API. As the first exercise, we'll create an account that will enable us to view (almost) all the resources in the `default` Namespace. The definition is available in the `sa/view.yml` file.

```bash
cat sa/view.yml
```

The output is as follows.

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: view

---

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: view
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: view
subjects:
- kind: ServiceAccount
  name: view
```

The YAML defines two resources. The first one is the `ServiceAccount` named `view`. The ServiceAccount kind of resources is on pair with Namespace in its simplicity. Excluding a few flags which we won't explore just yet, the only thing we can do with it is to declare its existence. The real magic is defined in the RoleBinding.

Just as with RBAC users and groups, RoleBindings are tying Roles to ServiceAccounts. Since our objective to provide read-only permissions that can be fulfilled with the ClusterRole `view`, we did not need to create a new Role. Instead, we're binding the ClusterRole `view` with the ServiceAccount with the same name.

If you are already experienced with RBAC applied to users and groups, you probably noticed that ServiceAccounts follow the same pattern. The only substantial difference, from YAML perspective, is that the `kind` of the subject is now `ServiceAccount`, instead of being `User` or `Group`.

Let's create the YAML and observe the results.

```bash
kubectl apply -f sa/view.yml --record
```

The output should show that the ServiceAccount and the RoleBinding were created.

Next, we'll list the ServiceAccounts in the `default` Namespace and confirm that the new one was created.

```bash
kubectl get sa
```

The output is as follows.

```
NAME    SECRETS AGE
default 1       27m
view    1       6s
```

Let's take a closer look at the ServiceAccount `view` we just created.

```bash
kubectl describe sa view
```

The output is as follows.

```
Name:        view
Namespace:   default
Labels:      <none>
Annotations: kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"v1","kind":"ServiceAccount","metadata":{"annotations":{},"name":"view","namespace":"default"}}
             kubernetes.io/change-cause=kubectl apply --filename=sa/view.yml --record=true
Image pull secrets: <none>
Mountable secrets:  view-token-292vm
Tokens:             view-token-292vm
Events:             <none>
```

There's not much to look at since, as we already saw from the definition, ServiceAccounts are only placeholders for bindings.

We should be able to get more information from the binding.

```bash
kubectl describe rolebinding view
```

The output is as follows.

```
Name:        view
Labels:      <none>
Annotations: kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"rbac.authorization.k8s.io/v1beta1","kind":"RoleBinding","metadata":{"annotations":{},"name":"view","namespace":"default"},"roleRef":{"ap...
             kubernetes.io/change-cause=kubectl apply --filename=sa/view.yml --record=true
Role:
  Kind: ClusterRole
  Name: view
Subjects:
  Kind           Name Namespace
  ----           ---- ---------
  ServiceAccount view
```

We can see that the `ClusterRole` named `view` has a single subject with the ServiceAccount.

Now that we have the ServiceAccount that has enough permissions to view (almost) all the resources, we'll create a Pod with `kubectl` which we can use to explore permissions.

The definition is in the `sa/kubectl-view.yml` file.

```bash
cat sa/kubectl-view.yml
```

The output is as follows.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: kubectl
spec:
  serviceAccountName: view
  containers:
  - name: kubectl
    image: vfarcic/kubectl
    command: ["sleep"]
    args: ["100000"]
```

The only new addition is the `serviceAccountName: view` entry. It associates the Pod with the account.

Let's create the Pod and describe it.

```bash
kubectl apply \
    -f sa/kubectl-view.yml \
    --record

kubectl describe pod kubectl
```

The relevant parts of the output of the latter command are as follows.

```yaml
Name: kubectl
...
Containers:
  kubectl:
    ...
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from view-token-292vm (ro)
...
Volumes:
  view-token-292vm:
    Type:       Secret (a volume populated by a Secret)
    SecretName: view-token-292vm
...
```

Since we declared that we want to associate the Pod with the account `view`, Kubernetes mounted a token to the container. If we defined more containers in that Pod, all would have the same mount.

Further on, we can see that the mount is using a Secret. It contains the same file structure we observed earlier with the `default` account.

Let's see whether we can retrieve the Pods from the same Namespace.

```bash
kubectl exec -it kubectl -- sh

kubectl get pods
```

We entered into the Shell of the container and listed the Pods.

The output is as follows.

```
NAME    READY STATUS  RESTARTS AGE
kubectl 1/1   Running 0        55s
```

Now that the Pod is associated with the ServiceAccount that has `view` permissions, we can indeed list the Pods. At the moment, we can see the `kubectl` Pod since it is the only one running in the `default` Namespace.

W> ## A note to minishift users
W>
W> You should see three Pods instead of one. When we created the OpenShift cluster with minikube, it deployed Docker Registry and a Router to the `default` Namespace.

Even though you probably know the answer, we'll confirm that we can indeed only view the resources and that all other operations are forbidden. We'll try to create a new Pod.

```bash
kubectl run new-test \
    --image=alpine \
    --restart=Never \
    sleep 10000
```

The output is a follows.

```
Error from server (Forbidden): pods is forbidden: User "system:serviceaccount:default:view" cannot create pods in the namespace "default"
```

As expected, we cannot create Pods. Since we bound the ClusterRole `view` to the ServiceAccount, and that allows us only read-only access to resources.

We'll exit the container and delete the resources we created before we continue exploring ServiceAccounts.

```bash
exit

kubectl delete -f sa/kubectl-view.yml
```

Jenkins' Kubernetes plugin needs to have full permissions related to Pods. It should be able to create them, to retrieve logs from them, to execute processes, to delete them, and so on. The resources defined in `sa/pods.yml` should allow us just that.

```bash
cat sa/pods.yml
```

The output is as follows.

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: pods-all
  namespace: test1

---

kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: pods-all
  namespace: test1
rules:
- apiGroups: [""]
  resources: ["pods", "pods/exec", "pods/log"]
  verbs: ["*"]

---

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: pods-all
  namespace: test1
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: pods-all
subjects:
- kind: ServiceAccount
  name: pods-all
```

Just as before, we are creating a ServiceAccount. This time we're naming it `pods-all`. Since none of the existing roles provide the types of privileges we need, we're defining a new one that provides full permissions with Pods. Finally, the last resource is the binding that ties the two together. All three resources are, this time, defined in the Namespace `test1`.

Let's create the resources.

```bash
kubectl apply -f sa/pods.yml \
    --record
```

So far, we did not explore the effect of ServiceAccounts to Namespaces other than those where we created them, so we'll create another one called `test2`.

```bash
kubectl create ns test2
```

Finally, we'll run `kubectl` as a Pod so that we can test the new account. Let's take a very quick look at the updated `kubectl` Pod definition.

```bash
cat sa/kubectl-test1.yml
```

The output is as follows.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: kubectl
  namespace: test1
spec:
  serviceAccountName: pods-all
  containers:
  - name: kubectl
    image: vfarcic/kubectl
    command: ["sleep"]
    args: ["100000"]
```

The only differences from the previous `kubectl` definition are in the `namespace` and the `serviceAccountName`. I'll assume that there's no need for an explanation. Instead, we'll proceed and `apply` the definition.

```bash
kubectl apply \
    -f sa/kubectl-test1.yml \
    --record
```

Now we're ready to test the new account.

```bash
kubectl -n test1 exec -it kubectl -- sh

kubectl get pods
```

We entered the `kubectl` container and retrieved the Pods.

The output is as follows.

```
NAME    READY STATUS  RESTARTS AGE
kubectl 1/1   Running 0        5m
```

We already experienced the same result when we used the ServiceAccount with the `view` Role, so let's try something different and try to create a Pod.

```bash
kubectl run new-test \
    --image=alpine \
    --restart=Never \
    sleep 10000
```

Unlike before, this time the `pod "new-test"` was `created`. We can confirm that by listing the Pods one more time.

```bash
kubectl get pods
```

The output is as follows.

```
NAME     READY STATUS  RESTARTS AGE
kubectl  1/1   Running 0        6m
new-test 1/1   Running 0        17s
```

How about creating a Deployment?

```bash
kubectl run new-test \
    --image=alpine sleep 10000
```

As you hopefully already know, if we execute a `run` command without the `--restart=Never` argument, Kubernetes creates a Deployment, instead of a Pod.

The output is as follows.

```
Error from server (Forbidden): deployments.extensions is forbidden: User "system:serviceaccount:test1:pods-all" cannot create deployments.extensions in the namespace "test1"
```

It's obvious that our ServiceAccount does not have any permissions aside from those related to Pods, so we were forbidden from creating a Deployment.

Let's see what happens if, for example, we try to retrieve the Pods from the Namespace `test2`. As a reminder, we are still inside a container that forms the Pod in the `test1` Namespace.

```bash
kubectl -n test2 get pods
```

The ServiceAccount was created in the `test1` Namespace. Therefore only the Pods created in the same Namespace can be attached to the `pods-all` ServiceAccount. In this case, the principal thing to note is that the RoleBinding that gives us the permissions to, for example, retrieve the Pods, exists only in the `test1` Namespace. The moment we tried to retrieve the Pods from a different Namespace, the API server responded with an error notifying us that we do not have permissions to `list pods in the namespace "test2"`.

T> While user accounts are global, ServiceAccounts are namespaced. After all, a human user is almost always invoking the API from outside the cluster while the processes inside containers are always inside the Pods which are inside the Namespaces.

We'll exit the container and delete the resources we created, before we explore other aspects of ServiceAccounts.

```bash
exit

kubectl delete -f sa/kubectl-test1.yml
```

We saw that, with the previous definition, we obtained permissions only within the same Namespace as the Pod attached to the ServiceAccount. In some cases that is not enough. Jenkins is a good use-case. We might decide to run Jenkins master in one Namespace but run the builds in another. Or, we might create a pipeline that deploys a beta version of our application and tests it in one Namespace and, later on, deploys it as production release in another. Surely there is a way to accommodate such needs.

Let's take a look at yet another YAML.

```bash
cat sa/pods-all.yml
```

The output is as follows.

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: pods-all
  namespace: test1

---

kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: pods-all
  namespace: test1
rules:
- apiGroups: [""]
  resources: ["pods", "pods/exec", "pods/log"]
  verbs: ["*"]

---

kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: pods-all
  namespace: test2
rules:
- apiGroups: [""]
  resources: ["pods", "pods/exec", "pods/log"]
  verbs: ["*"]

---

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: pods-all
  namespace: test1
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: pods-all
subjects:
- kind: ServiceAccount
  name: pods-all

---

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: pods-all
  namespace: test2
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: pods-all
subjects:
- kind: ServiceAccount
  name: pods-all
  namespace: test1
```

We start by creating a ServiceAccount and a Role called `pods-all` in the `test1` Namespace. Further on, we're creating the same role but in the `test2` Namespace. Finally, we're creating RoleBindings in both Namespaces. The only difference between the two is in a single line. The RoleBinding in the `test2` Namespace has the `subjects` entry `namespace: test1`. With it, we are linking a binding from one Namespace to a ServiceAccount in another. As a result, we should be able to create a Pod in the `test1` Namespace that will have full permissions to operate Pods in both Namespaces.

The two Roles could have been with different permission. They are the same (for now) only for simplicity reasons. We could have simplified the definition by defining a single ClusterRole and save ourselves from having two Roles (one in each Namespace). However, once we get back to Jenkins, we'll probably want to have different permissions in different Namespaces, so we're defining two Roles as a practice.

Let's apply the new definition, create a `kubectl` Pod again, and enter inside its only container.

```bash
kubectl apply -f sa/pods-all.yml \
    --record

kubectl apply \
    -f sa/kubectl-test2.yml \
    --record

kubectl -n test1 exec -it kubectl -- sh
```

There's probably no need to confirm again that we can retrieve the Pods from the same Namespace so we'll jump straight to testing whether we can now operate within the `test2` Namespace.

```bash
kubectl -n test2 get pods
```

The output shows that `no resources` were `found`. If we did not have permissions to view the files, we'd get the already familiar `forbidden` message instead.

To be on the safe side, we'll try to create a Pod in the `test2` Namespace.

```bash
kubectl -n test2 \
    run new-test \
    --image=alpine \
    --restart=Never \
    sleep 10000

kubectl -n test2 get pods
```

The output of the latter command is as follows.

```
NAME     READY STATUS  RESTARTS AGE
new-test 1/1   Running 0        18s
```

The Pod was created in the `test2` Namespace. Mission was accomplished, and we can get out of the container and delete the Namespaces we created, before we try to apply the knowledge about ServiceAccounts to Jenkins.

```bash
exit

kubectl delete ns test1 test2
```

## Configuring Jenkins Kubernetes Plugin With ServiceAccounts

Now that we got a grip on ServiceAccounts, it should be relatively straightforward to correct the problem we experienced with Jenkins. As a reminder, we could not configure the Kubernetes plugin. We experienced the same `forbidden` message as when we tried to use `kubectl` container with the `default` ServiceAccount. Now that we know that ServiceAccounts provide permissions to processes running inside containers, all we have to do is to define one for Jenkins.

We'll spice it up a bit with a slightly more complicated use-case. We'll try to run Jenkins master in one Namespace and perform builds in another. That way we can have a clear separation between Jenkins and "random" stuff our builds might be doing. Through such separation, we can guarantee that Jenkins will (probably) not be affected if we do something wrong in our builds.

W> Our example will not set up LimitRanges and ResourceQuotas. I'll assume that you're familiar with them and that you understand that a more serious setting must have them defined. If you're a newbie to those things, please consult the official documentation or explore related chapters in [The DevOps 2.3 Toolkit: Kubernetes](https://amzn.to/2GvzDjy).

Let's take a look at yet another Jenkins definition.

```bash
cat sa/jenkins.yml
```

The relevant parts of the output are as follows.

```yaml
...
apiVersion: v1
kind: Namespace
metadata:
  name: build

---

apiVersion: v1
kind: ServiceAccount
metadata:
  name: jenkins
  namespace: jenkins

---

kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: jenkins
  namespace: build
rules:
- apiGroups: [""]
  resources: ["pods", "pods/exec", "pods/log"]
  verbs: ["*"]
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get"]

---

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: jenkins
  namespace: build
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: jenkins
subjects:
- kind: ServiceAccount
  name: jenkins
  namespace: jenkins
...
apiVersion: apps/v1beta2
kind: StatefulSet
metadata:
  name: jenkins
  namespace: jenkins
spec:
  ...
  template:
    ...
    spec:
      serviceAccountName: jenkins
      ...
```

This time we are creating a second Namespace called `build` and a ServiceAccount `jenkins` in the `jenkins` namespace. Further on, we created a Role and a RoleBinding that provides required permissions in the `build` Namespace. As such, we bound the Role with the ServiceAccount in the `jenkins` Namespace. As a result, Jenkins should be able to create Pods in `build`, but it won't be able to do anything in its own Namespace `jenkins`. That way we can be relatively safe that a problem in our builds will not affect Jenkins. If we specified ResourceQuotas and LimitRanges for both Namespaces, our solution would be even more bulletproof. But, since I assume that you know how to do that, I excluded them in an attempt to simplify the definition.

Let's apply the config and observe the result.

W> ## A note to minishift users
W>
W> Due to the changes required for OpenShift, we'll use a different YAML specification. Please execute `oc apply -f sa/jenkins-oc.yml --record` instead of the command that follows.

```bash
kubectl apply \
    -f sa/jenkins.yml \
    --record
```

W> ## A note to GKE users
W>
W> GKE uses external load balancer as Ingress. To work properly, the `type` of the service related to Ingress needs to be `NodePort`. We'll have to patch the service to change its type. Please execute the command that follows.
W>
W> `kubectl -n jenkins patch svc jenkins -p '{"spec":{"type": "NodePort"}}'`

We can see from the output that the resources were created. All that's left is to wait until Jenkins rolls out.

```bash
kubectl -n jenkins \
    rollout status sts jenkins
```

Now that Jenkins is up-and-running, we should open it in a browser and repeat the same setup steps we did before.

```bash
open "http://$CLUSTER_DNS/jenkins"
```

The first step is to get the initial admin password.

```bash
kubectl -n jenkins \
    exec jenkins-0 -it -- \
    cat /var/jenkins_home/secrets/initialAdminPassword
```

Please copy the output and paste it into the *Administrator password* field. Click *Continue*, followed by the *Install suggested plugins* button. The rest of the setup requires you to *Create First Admin User*, so please go ahead. You don't need my help on that one.

Just as before, we'll need to add *Kubernetes* and *BlueOcean* plugins.

```bash
open "http://$CLUSTER_DNS/jenkins/pluginManager/available"
```

You already know what to do. Once you're done installing the two plugins, we'll go to the configuration screen.

```bash
open "http://$CLUSTER_DNS/jenkins/configure"
```

Please expand the *Add a new cloud* drop-down list in the *Cloud* section and select *Kubernetes*.

Now that we have the ServiceAccount that grants us the required permissions, we can click the *Test Connection* button and confirm that it works.

The output is as follows.

```
Error testing connection : Failure executing: GET at: https://kubernetes.default.svc/api/v1/namespaces/jenkins/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods is forbidden: User "system:serviceaccount:jenkins:jenkins" cannot list pods in the namespace "jenkins".
```

Did we do something wrong? We didn't. That was the desired behavior. By not specifying a Namespace, Jenkins checked whether it has necessary permission in the Namespace where it runs. If we try to invoke Kube API from a container, it'll always use the same Namespace as the one where the container is. Jenkins is no exception. On the other hand, our YAML explicitly defined that we should have permissions to create Pods in the `build` Namespace. Let's fix that.

Please type *build* in the *Kubernetes Namespace* field and click the *Test Connection* button again.

This time the output shows that the `connection test` was `successful`. We managed to configure Jenkins' Kubernetes plugin to operate inside the `build` Namespace. Still, there is one more thing missing.

When we create a job that uses Kubernetes Pods, an additional container will be added. That container will use JNLP to establish communication with the Jenkins master. We need to specify a valid address JNLP can use to connect to the master. Since the Pods will be in the `build` Namespace and the master is in `jenkins`, we need to use the longer DNS name that specifies both the name of the service (`jenkins`) as well as the Namespace (also `jenkins`). On top of all that, our master is configured to respond to requests with the root path `/jenkins`. All the all, the full address Pods can use to communicate with Jenkins master is should be `http://[SERVICE_NAME].[NAMESPACE]/[PATH]`. Since all three of those elements are `jenkins`, the "real" address is `http://jenkins.jenkins/jenkins`. Please type it inside the *Jenkins URL* field and click the *Save* button.

Now we're ready to create a job that'll test that everything works as expected.

Please click the *New Item* link from the left-hand menu to open a screen for creating jobs. Type *my-k8s-job* in the *item name* field, select *Pipeline* as the type, and click the *OK* button. Once inside the job configuration screen, click the *Pipeline* tab and write the script that follows inside the *Pipeline Script* field.

```groovy
podTemplate(
    label: 'kubernetes',
    containers: [
        containerTemplate(name: 'maven', image: 'maven:alpine', ttyEnabled: true, command: 'cat'),
        containerTemplate(name: 'golang', image: 'golang:alpine', ttyEnabled: true, command: 'cat')
    ]
) {
    node('kubernetes') {
        container('maven') {
            stage('build') {
                sh 'mvn --version'
            }
            stage('unit-test') {
                sh 'java -version'
            }
        }
        container('golang') {
            stage('deploy') {
                sh 'go version'
            }
        }
    }
}
```

I> If you prefer to copy and paste, the job is available in the [my-k8s-job.groovy Gist](https://gist.github.com/2cf872c3a9acac51409fbd5a2789cb02).

The job is relatively simple. It uses `podTemplate` to define a `node` that will contain two containers. One of those is `golang`, and the other is `maven`. In both cases the `command` is `cat`. Without a long-running command (process `1`), the container would exit immediately, Kubernetes would detect that and start another container based on the same image. It would fail again, and the loop would continue. Without the main process running, we'd enter into a never-ending loop.

Further on, we are defining that we want to use the `podTemplate` as a node and we start executing `sh` commands in different containers. Those commands only output software versions. The goal of this job is not to demonstrate a full CD pipeline (we'll do that later), but only to prove that integration with Kubernetes works and that we can use different containers that contain the tools we need.

Don't forget to click the *Save* button.

Now that we have a job, we should run it and validate that the integration with Kubernetes indeed works.

Please click the *Open Blue Ocean* link from the left-hand menu followed by the *Run* button.

We'll let Jenkins run the build and switch to Shell to observe what's happening.

```bash
kubectl -n build get pods
```

After a while, the output should be as follows.

```
NAME              READY STATUS            RESTARTS AGE
jenkins-slave-... 0/3   ContainerCreating 0        11s
```

We can see that Jenkins created a Pod with three containers. At this moment in time, those containers are still not fully functional. Kubernetes is probably pulling them to the assigned node.

You might be wondering why are there three containers even though we specified two. Jenkins added the third to the Pod definition. It contains JNLP that is in charge of communication between Pods acting as nodes and Jenkins masters. From user's perspective, JNLP is non-existent. It is a transparent process we do not need to worry about.

Let's take another look at the Pods in the `build` Namespace.

```bash
kubectl -n build get pods
```

The output is as follows.

```
NAME              READY STATUS  RESTARTS AGE
jenkins-slave-... 3/3   Running 0        5s
```

This time, if you are a fast reader, all the containers that form the Pod are running, and Jenkins is using them to execute the instructions we defined in the job.

Let's take another look at the Pods.

```bash
kubectl -n build get pods
```

The output is as follows.

```
NAME              READY STATUS      RESTARTS AGE
jenkins-slave-... 3/3   Terminating 0        32s
```

Once Jenkins finished executing the instructions from the job, it issued a command to Kube API to terminate the Pod.

Jenkins nodes created through `podTemplate` are called on-shot agents. Instead of having long-running nodes, they are created when needed and destroyed when not in use. Since they are Pods, Kubernetes is scheduling them on the nodes that have enough resources. By combining one-shot agents with Kubernetes, we are distributing load and, at the same time, using only the resources we need. After all, there's no need to waste CPU and memory on non-existing processes.

We're done with Jenkins, so let's remove the Namespaces we created before we move into the next use-case.

```bash
kubectl delete ns jenkins build
```

## Using ServiceAccounts From Side-Car Containers

We still have one more pending issue that we can solve with ServiceAccounts. In the previous chapter we tried to use `cvallance/mongo-k8s-sidecar` container in hopes it'll dynamically create and manage a MongoDB replica set. We failed because, at that time, we did not know how to create sufficient permissions that would allow the side-car to do its job. Now we know better.

Let's take a look at an updated version of our *go-demo-3* application.

```bash
cat sa/go-demo-3.yml
```

The relevant parts of the output are as follows

```yaml
...
apiVersion: v1
kind: ServiceAccount
metadata:
  name: db
  namespace: go-demo-3

---

kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: db
  namespace: go-demo-3
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["list"]

---

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: db
  namespace: go-demo-3
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: db
subjects:
- kind: ServiceAccount
  name: db

---

apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: db
  namespace: go-demo-3
spec:
  ...
  template:
    ...
    spec:
      serviceAccountName: db
      ...
```

Just as with Jenkins, we have a ServiceAccount, a Role, and a RoleBinding. Since the side-car needs only to list the Pods, the Role is this time more restrictive than the one we created for Jenkins. Further down, in the StatefulSet, we added `serviceAccountName: db` entry that links the set with the account. By now, you should be familiar with all those resources. We're applying the same logic to the side-car as to Jenkins.

Since there's no need for a lengthy discussion, we'll move on and `apply` the definition.

```bash
kubectl apply \
    -f sa/go-demo-3.yml \
    --record
```

Next, we'll take a look at the Pods created in the `go-demo-3` Namespace.

```bash
kubectl -n go-demo-3 \
    get pods
```

After a while, the output should be as follows.

```
NAME    READY STATUS  RESTARTS AGE
api-... 1/1   Running 1        1m
api-... 1/1   Running 1        1m
api-... 1/1   Running 1        1m
db-0    2/2   Running 0        1m
db-1    2/2   Running 0        1m
db-2    2/2   Running 0        54s
```

All the Pods are running so it seems that, this time, the side-car did not have trouble communicating with the API.

To be on the safe side, we'll output the logs of one of the side-car containers.

```bash
kubectl -n go-demo-3 \
    logs db-0 -c db-sidecar
```

The output, limited to the last entries, is as follows.

```json
...
     { _id: 1,
       host: 'db-1.db.go-demo-3.svc.cluster.local:27017',
       arbiterOnly: false,
       buildIndexes: true,
       hidden: false,
       priority: 1,
       tags: {},
       slaveDelay: 0,
       votes: 1 },
     { _id: 2, host: 'db-2.db.go-demo-3.svc.cluster.local:27017' } ],
  settings:
   { chainingAllowed: true,
     heartbeatIntervalMillis: 2000,
     heartbeatTimeoutSecs: 10,
     electionTimeoutMillis: 10000,
     catchUpTimeoutMillis: 2000,
     getLastErrorModes: {},
     getLastErrorDefaults: { w: 1, wtimeout: 0 },
     replicaSetId: 5aef9e4c52b968b72a16ea5b } }
```

The details behind the output are not that important. What matters is that there are no errors. The side-car managed to retrieve the information it needs from Kube API, and all that's left for us is to delete the Namespace and conclude the chapter.

```bash
kubectl delete ns go-demo-3
```

## What Now?

ServiceAccounts combined with Roles and RoleBindings are an essential component for continuous deployment or any other process that needs to communicate with Kubernetes. The alternative is to run an unsecured cluster which is not an option for any but smallest organizations. RBAC is required when more than one person is operating or using a cluster. If RBAC is enabled, ServiceAccounts are a must. We'll use them a lot in the chapters that follow.

Please consult the APIs that follow for any additional information about ServiceAccounts and related resources.

* [ServiceAccount v1 core](https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#serviceaccount-v1-core)]
* [Role v1 rbac](https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#role-v1-rbac)
* [ClusterRole v1 rbac](https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#clusterrole-v1-rbac)
* [RoleBinding v1 rbac](https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#rolebinding-v1-rbac)
* [ClusterRoleBinding v1 rbac](https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#clusterrolebinding-v1-rbac)

One more thing before you leave. Please consult [jenkins-kubernetes-plugin](https://github.com/jenkinsci/kubernetes-plugin) for more information about the plugin.
